{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7866f4-9264-4a25-85d2-a786a27ed55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import glob\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6b315-707a-4de2-ae1b-2b528bf5b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"INSERT_TOKEN\"\n",
    "headers = {\"Authorization\": f\"token {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e438208-7060-4802-915c-f04cf3d7f01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_CSV = r'output_files/fix_prs.csv'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    # --- PATH 1: FILE ALREADY EXISTS ---\n",
    "    print(f\"File {OUTPUT_CSV} found. Reading data from disk...\")\n",
    "    all_fix_prs = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "else:\n",
    "    # --- PATH 2: FILE DOES NOT EXIST ---\n",
    "    print(f\"File {OUTPUT_CSV} not found.\")\n",
    "    # --- Your original code to load and process ---\n",
    "    aidev_pop = load_dataset(\"hao-li/AIDev\", \"pull_request\")\n",
    "    pandas_aidev_pop = aidev_pop['train'].to_pandas()\n",
    "    \n",
    "    task_types = load_dataset(\"hao-li/AIDev\", \"pr_task_type\")\n",
    "    pandas_task_types = task_types['train'].to_pandas()\n",
    "    pandas_task_types.rename(columns={'confidence': 'type_confidence'}, inplace=True)\n",
    "    aidev_pop_with_types = pd.merge(pandas_aidev_pop, pandas_task_types[['id','type','type_confidence']], on='id')\n",
    "\n",
    "    human_prs = load_dataset(\"hao-li/AIDev\", \"human_pull_request\")\n",
    "    pandas_human_prs = human_prs['train'].to_pandas()\n",
    "\n",
    "    human_task_types = load_dataset(\"hao-li/AIDev\", \"human_pr_task_type\")\n",
    "    pandas_human_task_types = human_task_types['train'].to_pandas()\n",
    "    pandas_human_task_types.rename(columns={'confidence': 'type_confidence'}, inplace=True)\n",
    "    human_prs_with_types = pd.merge(pandas_human_prs, pandas_human_task_types[['id','type']], on='id')\n",
    "    fix_human_prs = human_prs_with_types[human_prs_with_types['type'] == 'fix']\n",
    "    fix_agent_prs = aidev_pop_with_types[aidev_pop_with_types['type'] == 'fix']\n",
    "    \n",
    "    cols_to_keep = ['id','number','user','user_id','agent','title','body','state',\n",
    "                    'created_at','closed_at','merged_at','repo_url','html_url']\n",
    "    \n",
    "    all_fix_prs = pd.concat([\n",
    "        fix_human_prs[cols_to_keep],\n",
    "        fix_agent_prs[cols_to_keep]\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        all_fix_prs.to_csv(OUTPUT_CSV, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save CSV file. Error: {e}\")\n",
    "all_fix_prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1d43f-9d55-440a-880d-2c274dff3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract possible issues from text\n",
    "def extract_issue_numbers(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    # Looks for patterns like #123 or owner/repo#123\n",
    "    pattern = r'(?:\\b[\\w-]+/[\\w-]+)?#(\\d+)\\b'\n",
    "    return [int(m) for m in re.findall(pattern, text)]\n",
    "\n",
    "# Applying to dataframe\n",
    "all_fix_prs['possible_issues'] = all_fix_prs.apply(\n",
    "    lambda row: extract_issue_numbers(str(row['title']) + ' ' + str(row['body'])),\n",
    "    axis=1\n",
    ")\n",
    "all_fix_prs['possible_issues'] = all_fix_prs['possible_issues'].apply(lambda x: list(set(x)) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ad3db-4ae3-40ae-90a9-577e7c1d1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- MODIFIED HELPER FUNCTION -----\n",
    "def _get_issue_type(url: str, session: requests.Session) -> str:\n",
    "    \"\"\"\n",
    "    Makes a GET call to the /issues API and determines if\n",
    "    it is a pure Issue, a Pull Request, or does not exist.\n",
    "    Returns: 'issue', 'pull_request', 'not_found'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # CRUCIAL CHANGE: session.get() instead of session.head()\n",
    "        # We need the JSON body to check for the 'pull_request' field\n",
    "        response = session.get(url, timeout=10) \n",
    "        \n",
    "        # 404 Not Found or 410 Gone = Does not exist\n",
    "        if response.status_code in (404, 410):\n",
    "            return 'not_found'\n",
    "        \n",
    "        # Handle Rate Limit (same as original)\n",
    "        if response.status_code == 403 and 'X-RateLimit-Remaining' in response.headers:\n",
    "            if int(response.headers['X-RateLimit-Remaining']) == 0:\n",
    "                print(\"Rate limit reached. Waiting 15 minutes...\")\n",
    "                time.sleep(15 * 60)\n",
    "                return _get_issue_type(url, session) # Retry\n",
    "\n",
    "        # Other errors (500, 403-Forbidden, etc.)\n",
    "        if response.status_code != 200:\n",
    "            return 'not_found' # Treat other errors as 'not_found'\n",
    "\n",
    "        # --- SUCCESS (200 OK) ---\n",
    "        data = response.json()\n",
    "        \n",
    "        # THE CHECK YOU REQUESTED:\n",
    "        if 'pull_request' in data:\n",
    "            return 'pull_request' # It's a PR\n",
    "        else:\n",
    "            return 'issue' # It's a pure Issue\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        return 'not_found' # Network error\n",
    "        \n",
    "\n",
    "# ----- MODIFIED MAIN FUNCTION -----\n",
    "\n",
    "def validar_issues_do_repo(df: pd.DataFrame, github_token: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Receives a DataFrame with 'repo_url' and 'possible_issues' (list)\n",
    "    and returns the DataFrame with a new column 'matched_issues'\n",
    "    containing only the numbers of PURE Issues (not PRs).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting refactoring: Optimizing API calls...\")\n",
    "    \n",
    "    # --- 1. SESSION AND CACHE SETUP ---\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'Authorization': f'token {github_token}',\n",
    "        'Accept': 'application/vnd.github.v3+json',\n",
    "        'X-GitHub-Api-Version': '2022-11-28'\n",
    "    })\n",
    "    \n",
    "    # Cache stores the TYPE of the issue\n",
    "    # Format: {(repo_url, issue_num): str} (e.g., 'issue', 'pull_request')\n",
    "    validation_cache = {}\n",
    "\n",
    "    # --- 2. FIND UNIQUE CHECKS (No changes) ---\n",
    "    \n",
    "    df_clean = df.dropna(subset=['repo_url', 'possible_issues'])\n",
    "    df_exploded = df_clean.explode('possible_issues')\n",
    "    df_exploded = df_exploded.dropna(subset=['possible_issues'])\n",
    "    df_exploded['possible_issues'] = df_exploded['possible_issues'].astype(int)\n",
    "    checks_needed = df_exploded[['repo_url', 'possible_issues']].drop_duplicates()\n",
    "    \n",
    "    print(f\"Optimization: {len(df)} rows reduced to {len(checks_needed)} unique API calls.\")\n",
    "\n",
    "    # --- 3. EXECUTE API CHECKS (MODIFIED) ---\n",
    "    \n",
    "    for _, row in tqdm(checks_needed.iterrows(), total=len(checks_needed), desc=\"Validating issues via API\"):\n",
    "        repo_url = row['repo_url']\n",
    "        issue_num = row['possible_issues']\n",
    "        cache_key = (repo_url, issue_num)\n",
    "        \n",
    "        if issue_num == 0:\n",
    "            validation_cache[cache_key] = 'not_found' # CHANGED\n",
    "            continue\n",
    "            \n",
    "        api_url = f\"{repo_url}/issues/{issue_num}\"\n",
    "        \n",
    "        # Call our NEW helper and save the TYPE in cache\n",
    "        validation_cache[cache_key] = _get_issue_type(api_url, session) # CHANGED\n",
    "\n",
    "    # --- 4. MAP RESULTS BACK (MODIFIED) ---\n",
    "    \n",
    "    print(\"Mapping results back to DataFrame...\")\n",
    "    \n",
    "    def find_matches(row):\n",
    "        repo = row['repo_url']\n",
    "        issues = row['possible_issues']\n",
    "        \n",
    "        if not isinstance(issues, list) or pd.isna(repo):\n",
    "            return [] \n",
    "            \n",
    "        matched_list = []\n",
    "        for num in issues:\n",
    "            # Get the type ('issue', 'pull_request', 'not_found') from cache\n",
    "            cached_result = validation_cache.get((repo, num), 'not_found')\n",
    "            \n",
    "            # ADD TO LIST ONLY IF IT IS A PURE ISSUE\n",
    "            if cached_result == 'issue': # CHANGED\n",
    "                matched_list.append(num)\n",
    "        \n",
    "        return matched_list\n",
    "\n",
    "    # Apply mapping function (fast, as it only uses cache)\n",
    "    df['matched_issues'] = df.apply(find_matches, axis=1)\n",
    "    \n",
    "    print(\"Refactoring complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fb95f-1dcc-4e56-916a-255eaca72169",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PARQUET = r'output_files\\possibles_issues.parquet'\n",
    "if os.path.exists(OUTPUT_PARQUET):\n",
    "    # --- PATH 1: FILE ALREADY EXISTS ---\n",
    "    print(f\"File {OUTPUT_PARQUET} found. Reading data from disk...\")\n",
    "    \n",
    "    df_com_matches = pd.read_parquet(OUTPUT_PARQUET)\n",
    "    \n",
    "    # Add cleaning for list columns (the numpy.ndarray problem)\n",
    "    def clean_parquet_list(item):\n",
    "        if isinstance(item, list):\n",
    "            return item\n",
    "        if isinstance(item, np.ndarray):\n",
    "            return item.tolist() # Converts NumPy array to list\n",
    "        return [] # Converts None, NaN, etc., to empty list\n",
    "    # Clean columns we know are lists\n",
    "    list_cols = ['possible_issues', 'matched_issues']\n",
    "    for col in list_cols:\n",
    "        if col in df_com_matches.columns:\n",
    "            df_com_matches[col] = df_com_matches[col].apply(clean_parquet_list)\n",
    "\n",
    "else:\n",
    "    # --- PATH 2: FILE DOES NOT EXIST ---\n",
    "    print(f\"File {OUTPUT_PARQUET} not found.\")\n",
    "    print(\"Starting issue validation via API (may take a while)...\")\n",
    "    df_com_matches = validar_issues_do_repo(all_fix_prs, token)\n",
    "    print(f\"API processing complete. Saving results to {OUTPUT_PARQUET}...\")\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(OUTPUT_PARQUET), exist_ok=True) \n",
    "        df_com_matches.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "        print(\"File saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save Parquet file. Error: {e}\")\n",
    "df_com_matches[['repo_url', 'possible_issues', 'matched_issues']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63534e73-d7d5-4ea7-9d9a-6084ab6aac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_BATCH_SIZE = 50 \n",
    "PARQUET_FILE = r'output_files\\linked_issues.parquet' \n",
    "CHECKPOINT_FILE = r'output_files\\linked_issues_checkpoint.json'\n",
    "\n",
    "# --- HELPER FUNCTION 1: Sub-Query Generator (No changes) ---\n",
    "def generate_pr_sub_query(pr_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Creates the GraphQL query part for a single PR using an alias.\n",
    "    Ex: pr123: pullRequest(number: 123) { ... }\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    pr{pr_number}: pullRequest(number: {pr_number}) {{\n",
    "      closingIssuesReferences(first: 10) {{\n",
    "        nodes {{\n",
    "          number\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "# --- HELPER FUNCTION 2: Optimized API Function (No changes) ---\n",
    "def get_linked_issues_batched(owner: str, repo: str, pr_numbers: list, headers: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches linked issues for a BATCH of PR numbers\n",
    "    from a SINGLE repository in ONE GraphQL call.\n",
    "    \"\"\"\n",
    "    graphql_url = \"https://api.github.com/graphql\"\n",
    "    \n",
    "    # 1. Build dynamic query\n",
    "    sub_queries = \"\\n\".join([generate_pr_sub_query(n) for n in pr_numbers])\n",
    "    query = f\"\"\"\n",
    "    query ($owner: String!, $repo: String!) {{\n",
    "      repository(owner: $owner, name: $repo) {{\n",
    "        {sub_queries}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    variables = {\"owner\": owner, \"repo\": repo}\n",
    "    \n",
    "    # Dictionary to store {pr_number: [issue_list]}\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(graphql_url, json={\"query\": query, \"variables\": variables}, headers=headers, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        \n",
    "        # 2. Handle partial GraphQL errors\n",
    "        if 'errors' in data:\n",
    "            print(f\"   GraphQL Error in {owner}/{repo}: {data['errors'][0]['message'][:100]}...\")\n",
    "            for num in pr_numbers:\n",
    "                results[num] = [] \n",
    "            return results\n",
    "            \n",
    "        repo_data = data.get('data', {}).get('repository')\n",
    "        if not repo_data:\n",
    "            raise Exception(\"'repository' not found in response.\")\n",
    "\n",
    "        # 3. Parse the response\n",
    "        for pr_alias, pr_data in repo_data.items():\n",
    "            pr_num = int(pr_alias[2:]) \n",
    "            \n",
    "            if pr_data is None:\n",
    "                results[pr_num] = []\n",
    "                continue\n",
    "                \n",
    "            nodes = pr_data.get('closingIssuesReferences', {}).get('nodes', [])\n",
    "            results[pr_num] = [n['number'] for n in nodes]\n",
    "            \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Network/Timeout Exception in {owner}/{repo} (Batch starting with PR {pr_numbers[0]}): {e}\")\n",
    "        for num in pr_numbers:\n",
    "            results[num] = []\n",
    "        return results\n",
    "\n",
    "# --- OPTIMIZED MAIN LOGIC (MODIFIED) ---\n",
    "\n",
    "# 1. Check if PARQUET FILE exists\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    print(\"Starting optimized search for linked issues...\")\n",
    "    \n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        print(f\"Loading progress from checkpoint: {CHECKPOINT_FILE}\")\n",
    "        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:\n",
    "            repo_results_map = json.load(f)\n",
    "    else:\n",
    "        repo_results_map = {}\n",
    "\n",
    "    repos_para_processar = df_com_matches[\n",
    "        ~df_com_matches['repo_url'].isin(repo_results_map.keys())\n",
    "    ]\n",
    "    grouped = repos_para_processar.groupby('repo_url')\n",
    "    \n",
    "    print(f\"Total repositories to process: {len(grouped)}\")\n",
    "    pbar = tqdm(grouped, desc=\"Processing Repositories\")\n",
    "\n",
    "    for repo_url, group in pbar:\n",
    "        try:\n",
    "            owner_repo = \"/\".join(repo_url.rstrip(\"/\").split(\"/\")[-2:])\n",
    "            owner, repo = owner_repo.split(\"/\")\n",
    "            pbar.set_postfix_str(f\"{owner}/{repo}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing repo URL {repo_url}: {e}. Skipping...\")\n",
    "            repo_results_map[repo_url] = {} \n",
    "            continue\n",
    "        \n",
    "        pr_numbers = group['number'].unique().tolist()\n",
    "        repo_batch_results = {} \n",
    "\n",
    "        for i in range(0, len(pr_numbers), PR_BATCH_SIZE):\n",
    "            batch_pr_numbers = pr_numbers[i:i+PR_BATCH_SIZE]\n",
    "            batch_results = get_linked_issues_batched(owner, repo, batch_pr_numbers, headers)\n",
    "            repo_batch_results.update(batch_results)\n",
    "\n",
    "        repo_results_map[repo_url] = repo_batch_results\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(repo_results_map, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"ALERT: Failed to save checkpoint: {e}\")\n",
    "\n",
    "    # --- END OF LOOP ---\n",
    "    \n",
    "    print(\"API processing complete. Mapping results back to DataFrame...\")\n",
    "\n",
    "    def map_results(row):\n",
    "        return repo_results_map.get(row['repo_url'], {}).get(row['number'], [])\n",
    "\n",
    "    df_com_matches['linked_issues'] = df_com_matches.apply(map_results, axis=1)\n",
    "    \n",
    "    # 7. Save final PARQUET (MODIFIED)\n",
    "    print(f\"Saving results to {PARQUET_FILE}...\")\n",
    "    # Parquet saves list [1, 2] as a list, not as string \"[1, 2]\"\n",
    "    df_com_matches.to_parquet(PARQUET_FILE, index=False)\n",
    "    \n",
    "    # Clear checkpoint\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "\n",
    "    df_com_issues_linkadas = df_com_matches.copy()\n",
    "else:\n",
    "    # --- PARQUET READING (MUCH SIMPLER) ---\n",
    "    print(f\"File {PARQUET_FILE} already exists, reading from disk.\")\n",
    "    df_com_issues_linkadas = pd.read_parquet(PARQUET_FILE)\n",
    "    \n",
    "    def clean_parquet_list(item):\n",
    "        # Case 1: Is a Python list (rare, but can happen)\n",
    "        if isinstance(item, list):\n",
    "            return item\n",
    "            \n",
    "        # Case 2: Is a NumPy array (most likely)\n",
    "        if isinstance(item, np.ndarray):\n",
    "            return item.tolist() # <-- Converts array to Python list\n",
    "            \n",
    "        # Case 3: Is None, NaN, or anything else\n",
    "        return [] # Returns empty list\n",
    "\n",
    "    df_com_issues_linkadas['linked_issues'] = df_com_issues_linkadas['linked_issues'].apply(clean_parquet_list)\n",
    "\n",
    "df_com_issues_linkadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc363f41-e702-4418-bc1e-91bbc9368397",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = ['id', 'number', 'repo_url']\n",
    "df_final = df_com_issues_linkadas.copy()\n",
    "df_final = df_final.set_index(key_cols)\n",
    "df_matched_indexed = df_com_matches.set_index(key_cols)\n",
    "\n",
    "df_final.update(df_matched_indexed[['possible_issues','matched_issues']])\n",
    "\n",
    "df_final = df_final.reset_index()\n",
    "\n",
    "df_final['issues'] = df_final.apply(\n",
    "    lambda row: list(set(row['matched_issues']) | set(row['linked_issues'])),\n",
    "    axis=1\n",
    ")\n",
    "prs_com_issue = df_final[df_final['issues'].str.len() > 0]\n",
    "prs_com_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07069e8c-1bff-4da3-b8c3-aa7a17ff1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = pd.read_csv(\"fixes_with_issues_linked_or_on_body.csv\")\n",
    "v1.rename(columns = {'linked_issues':'issues'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bf1c7-3d21-4c90-a423-ae785f4ba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_keys = v1[['id']].drop_duplicates()\n",
    "novo_keys = prs_com_issue[['id']].drop_duplicates()\n",
    "\n",
    "print(f\"Total unique PRs in v1 (Old): {len(v1_keys)}\")\n",
    "print(f\"Total unique PRs in prs_com_issue (New): {len(novo_keys)}\")\n",
    "print(\"-\" * 30)\n",
    "df_diff = pd.merge(\n",
    "    v1_keys,\n",
    "    novo_keys,\n",
    "    on='id',\n",
    "    how='outer',\n",
    "    indicator=True  # <-- Creates '_merge' column\n",
    ")\n",
    "\n",
    "print(\"Difference Analysis (PR Count):\")\n",
    "contagem = df_diff['_merge'].value_counts()\n",
    "print(contagem)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. NEW PRs (Found in 'new' but NOT in 'v1')\n",
    "novos_ids = df_diff[df_diff['_merge'] == 'right_only']['id']\n",
    "df_novos_encontrados = prs_com_issue[prs_com_issue['id'].isin(novos_ids)]\n",
    "\n",
    "# 2. LOST PRs (Were in 'v1' but NOT in 'new')\n",
    "perdidos_ids = df_diff[df_diff['_merge'] == 'left_only']['id']\n",
    "df_perdidos = v1[v1['id'].isin(perdidos_ids)]\n",
    "\n",
    "# 3. COMMON PRs (Were in both)\n",
    "comuns_ids = df_diff[df_diff['_merge'] == 'both']['id']\n",
    "df_comuns = prs_com_issue[prs_com_issue['id'].isin(comuns_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862ded7-9161-4e55-9f17-e9364fac74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comuns[['id','number','repo_url','issues']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d29c67-5af0-4c91-a9e1-2568cc80e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_chave = ['id', 'number', 'repo_url']\n",
    "\n",
    "print(\"Preparing source DataFrame (aggregating by keys)...\")\n",
    "df_source = df_comuns.groupby(\n",
    "    colunas_chave, \n",
    "    as_index=False\n",
    ")['issues'].first()\n",
    "\n",
    "# (Optional) Rename column to avoid conflict during merge\n",
    "df_source = df_source.rename(columns={'issues': 'common_issues'})\n",
    "\n",
    "\n",
    "# --- STEP 2: Execute Merge (Left Join) ---\n",
    "# 'how='left'' keeps all rows from 'df_com_issues_linkadas'\n",
    "# and adds data from 'df_source' where keys match.\n",
    "\n",
    "print(\"Executing merge (left join)...\")\n",
    "df_final = pd.merge(\n",
    "    df_com_issues_linkadas,\n",
    "    df_source,\n",
    "    on=colunas_chave,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Where there was no match, the new 'common_issues' column will have 'NaN'.\n",
    "\n",
    "\n",
    "# --- STEP 3: Clean NaNs (fillna with []) ---\n",
    "\n",
    "def clean_cell_to_list(x):\n",
    "    \"\"\"\n",
    "    Robust function that converts any cell to a clean Python list.\n",
    "    - Converts np.ndarray -> list\n",
    "    - Converts np.nan/None -> []\n",
    "    - Keeps list -> list\n",
    "    \"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [] # <-- Converts 'NaN' to '[]'\n",
    "\n",
    "print(\"Cleaning new column (converting NaNs to [])...\")\n",
    "df_final['common_issues'] = df_final['common_issues'].apply(clean_cell_to_list)\n",
    "df_final['has_issues'] = df_final.apply(lambda x: True if len(x['common_issues']) > 0 else False,axis = 1)\n",
    "df_final = df_final.rename(columns={'common_issues': 'issues'})\n",
    "df_final.drop(columns=['possible_issues','matched_issues','linked_issues'],inplace = True)\n",
    "# --- Verification ---\n",
    "print(\"Operation complete.\")\n",
    "print(f\"Final DataFrame size: {len(df_final)}\")\n",
    "print(\"\\nChecking rows that did NOT match (should have '[]'):\")\n",
    "display(df_final[df_final['issues'].apply(len) == 0].head())\n",
    "\n",
    "print(\"\\nChecking rows that DID match (should have lists):\")\n",
    "df_final[df_final['issues'].apply(len) > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d79ab3-c89d-488e-bcd0-52ccb75f1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7418e-4644-4533-8915-65750e200c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_parquet(r'output_files\\fix_PRs_with_issues.parquet',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8e559-7c2f-4498-8903-781e23d4c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_novos_encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039e6ac-7765-445c-9384-6ef22fa70bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
