{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cde684-7a1a-4750-9772-9ac9705fb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from IPython.display import display\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46568eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 1: Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def compare_acceptance_rates(series_group1, series_group2, alpha=0.05, label_1='group1', label_2='group2'):\n",
    "    \"\"\"\n",
    "    Compares two proportions (acceptance rates) using a Two-Proportion Z-Test.\n",
    "    \n",
    "    Parameters:\n",
    "    - series_group1 (pd.Series): Boolean or 0/1 series for group 1 (e.g., Agents).\n",
    "    - series_group2 (pd.Series): Boolean or 0/1 series for group 2 (e.g., Humans).\n",
    "    - alpha (float): Significance level.\n",
    "    - label_1 (str): Label for group 1.\n",
    "    - label_2 (str): Label for group 2.\n",
    "\n",
    "    Returns:\n",
    "    - (pd.DataFrame): DataFrame containing statistical results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle cases where a group might be empty\n",
    "    if series_group1.empty or series_group2.empty:\n",
    "        print(f\"Warning: Group {label_1} or {label_2} is empty. Z-Test not performed.\")\n",
    "        return None\n",
    "\n",
    "    count = [series_group1.sum(), series_group2.sum()]\n",
    "    nobs = [len(series_group1), len(series_group2)]\n",
    "    \n",
    "    # Avoid division by zero if a group has no observations\n",
    "    if nobs[0] == 0 or nobs[1] == 0:\n",
    "        print(f\"Warning: Group {label_1} ({nobs[0]} obs) or {label_2} ({nobs[1]} obs) has no observations. Z-Test not performed.\")\n",
    "        return None\n",
    "\n",
    "    # Handle value error if both groups have 0% or 100% acceptance\n",
    "    if (count[0] == 0 and count[1] == 0) or (count[0] == nobs[0] and count[1] == nobs[1]):\n",
    "         print(f\"Warning: Both groups have the same rate (0% or 100%). Z-Test not performed.\")\n",
    "         return None\n",
    "\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    \n",
    "    rate_group1 = count[0] / nobs[0]\n",
    "    rate_group2 = count[1] / nobs[1]\n",
    "    \n",
    "    significant = pval < alpha\n",
    "    \n",
    "    if significant:\n",
    "        interpretation = label_1 + \" significantly worse\" if stat < 0 else label_1 + \" significantly better\"\n",
    "    else:\n",
    "        interpretation = \"No significant difference\"\n",
    "        \n",
    "    result = {\n",
    "        f\"{label_1}_accept_rate\": rate_group1,\n",
    "        f\"{label_2}_accept_rate\": rate_group2,\n",
    "        \"z_stat\": stat,\n",
    "        \"p_value\": pval,\n",
    "        \"significant\": significant,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"absolute_difference\": rate_group1 - rate_group2,\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([result])\n",
    "\n",
    "def cliffs_delta(x, y):\n",
    "    \"\"\"\n",
    "    Calculates Cliff's Delta (d). Returns a value between -1 and 1.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    # Create a comparison matrix\n",
    "    comparisons = np.sign(x[:, None] - y)\n",
    "    \n",
    "    # Calculate the mean of these comparisons\n",
    "    d = np.mean(comparisons)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def compare_continuous_non_parametric(group1_series, group2_series, group1_name=\"Group 1\", group2_name=\"Group 2\"):\n",
    "    \"\"\"\n",
    "    Compares two independent non-parametric samples using Mann-Whitney U\n",
    "    and calculates effect size using Cliff's Delta.\n",
    "    \"\"\"\n",
    "    \n",
    "    if group1_series.empty or group2_series.empty:\n",
    "        print(\"Not enough data in one of the groups for Mann-Whitney U comparison.\")\n",
    "        return None\n",
    "\n",
    "    # 1. Mann-Whitney U Test\n",
    "    stat, p_value = stats.mannwhitneyu(group1_series, group2_series)\n",
    "    print(f\"Mann-Whitney U P-value: {p_value}\")\n",
    "\n",
    "    # 2. Effect Size (Cliff's Delta)\n",
    "    delta = cliffs_delta(group1_series, group2_series)\n",
    "\n",
    "    # Magnitude Interpretation\n",
    "    abs_delta = abs(delta)\n",
    "    if abs_delta < 0.147:\n",
    "        magnitude = \"Negligible\"\n",
    "    elif abs_delta < 0.33:\n",
    "        magnitude = \"Small\"\n",
    "    elif abs_delta < 0.474:\n",
    "        magnitude = \"Medium\"\n",
    "    else:\n",
    "        magnitude = \"Large\"\n",
    "\n",
    "    print(f\"\\\\n--- Effect Size (Cliff's Delta) ---\")\n",
    "    print(f\"Cliff's Delta: {delta:.4f}\")\n",
    "    print(f\"Interpretation: The difference size is considered '{magnitude}'.\")\n",
    "\n",
    "    # Direction Interpretation\n",
    "    if delta < 0:\n",
    "        print(f\"(The first group, '{group1_name}', tends to have LOWER values than the second, '{group2_name}')\")\n",
    "    elif delta > 0:\n",
    "         print(f\"(The first group, '{group1_name}', tends to have HIGHER values than the second, '{group2_name}')\")\n",
    "    else:\n",
    "        print(\"(The groups are perfectly overlapping)\")\n",
    "        \n",
    "    return {\n",
    "        \"mw_stat\": stat,\n",
    "        \"mw_p_value\": p_value,\n",
    "        \"cliffs_delta\": delta,\n",
    "        \"effect_size_magnitude\": magnitude\n",
    "    }\n",
    "\n",
    "def check_normality(data_series):\n",
    "    \"\"\"\n",
    "    Checks the normality of a data series using the Shapiro-Wilk test.\n",
    "    \"\"\"\n",
    "    data = data_series.dropna()\n",
    "    \n",
    "    if len(data) > 5000:\n",
    "        print(\"Warning: Over 5000 samples. Shapiro-Wilk p-value might not be accurate (using sample of 5000).\")\n",
    "        data = data.sample(5000)\n",
    "\n",
    "    if len(data) < 3:\n",
    "        print(\"Not enough data for normality test.\")\n",
    "        return None, \"Insufficient\"\n",
    "\n",
    "    stat, p_value = stats.shapiro(data)\n",
    "    \n",
    "    print(f\"Shapiro-Wilk Test P-value: {p_value}\")\n",
    "    if p_value > 0.05:\n",
    "        print(\"Data appears to be normally distributed.\")\n",
    "        return p_value, \"Normal\"\n",
    "    else:\n",
    "        print(\"Data is NOT normally distributed.\")\n",
    "        return p_value, \"Not Normal\"\n",
    "\n",
    "def analyze_odds_ratio(grouped_df, agent_label='Group1', human_label='Group2'):\n",
    "    \"\"\"\n",
    "    Calculates Odds Ratio from a grouped DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = grouped_df.loc[agent_label, 'accepted_count'] # Group 1 Accepted\n",
    "        b = grouped_df.loc[agent_label, 'rejected_count'] # Group 1 Rejected\n",
    "        c = grouped_df.loc[human_label, 'accepted_count'] # Group 2 Accepted\n",
    "        d = grouped_df.loc[human_label, 'rejected_count'] # Group 2 Rejected\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Label not found in grouped DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "    if b == 0 or c == 0:\n",
    "        print(f\"Error: Imminent division by zero (count 0 in '{agent_label}_rejected' or '{human_label}_accepted').\")\n",
    "        odds_ratio = np.inf if c == 0 else 0\n",
    "        inverse_or = 1 / odds_ratio if odds_ratio != 0 else np.inf\n",
    "    else:\n",
    "        odds_ratio = (a * d) / (b * c)\n",
    "        inverse_or = 1 / odds_ratio\n",
    "\n",
    "    print(f\"Odds Ratio ({agent_label} vs {human_label}): {odds_ratio:.4f}\")\n",
    "    print(f\"Inverse Odds Ratio ({human_label} vs {agent_label}): {inverse_or:.4f}\")\n",
    "    print(f\"\\\\nInterpretation: The odds of '{human_label}' being accepted are {inverse_or:.2f} times higher than '{agent_label}'.\")\n",
    "    \n",
    "    return {\n",
    "        \"odds_ratio\": odds_ratio,\n",
    "        \"inverse_odds_ratio\": inverse_or\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d54477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 2: THE NEW MASTER FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_full_comparison(df, group1_mask, group2_mask, \n",
    "                        group1_label, group2_label, \n",
    "                        analysis_title=\"Comparative Analysis\",\n",
    "                        acceptance_col='accepted',\n",
    "                        state_col='state',\n",
    "                        closed_value='closed',\n",
    "                        created_at_col='created_at',\n",
    "                        closed_at_col='closed_at'):\n",
    "    \"\"\"\n",
    "    Executes a full comparative analysis (Acceptance Rate, Odds Ratio, Duration)\n",
    "    between two groups defined by boolean masks.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - group1_mask (pd.Series): Boolean mask for Group 1 (e.g., df['user_type'] == 'Human').\n",
    "    - group2_mask (pd.Series): Boolean mask for Group 2 (e.g., df['user_type'] != 'Human').\n",
    "    - group1_label (str): Label for Group 1 (e.g., 'Human').\n",
    "    - group2_label (str): Label for Group 2 (e.g., 'Agent').\n",
    "    - analysis_title (str): Title for the analysis output.\n",
    "    - ... (column names)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=========================================================\")\n",
    "    print(f\"STARTING: {analysis_title}\")\n",
    "    print(f\"=========================================================\\\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # --- 1. Acceptance Rate Analysis (Z-Test) ---\n",
    "    print(f\"--- 1. Acceptance Rate Analysis ({group2_label} vs {group1_label}) ---\")\n",
    "    \n",
    "    # Ensure we are using only data from the original DataFrame\n",
    "    accepted_g1 = df.loc[group1_mask, acceptance_col]\n",
    "    accepted_g2 = df.loc[group2_mask, acceptance_col]\n",
    "    \n",
    "    # Pass g2 (e.g., Agent) as label_1 and g1 (e.g., Human) as label_2\n",
    "    acceptance_results_df = compare_acceptance_rates(\n",
    "        accepted_g2, accepted_g1, \n",
    "        label_1=group2_label, label_2=group1_label\n",
    "    )\n",
    "    \n",
    "    if acceptance_results_df is not None:\n",
    "        style_format = {\n",
    "            f\"{group2_label}_accept_rate\": \"{:.2%}\",\n",
    "            f\"{group1_label}_accept_rate\": \"{:.2%}\",\n",
    "            \"z_stat\": \"{:.2f}\",\n",
    "            \"p_value\": \"{:.2e}\",\n",
    "            \"absolute_difference\": \"{:.2%}\",\n",
    "        }\n",
    "        display(acceptance_results_df.style.format(style_format))\n",
    "        all_results['acceptance'] = acceptance_results_df\n",
    "    else:\n",
    "        print(\"Could not calculate acceptance rate.\\\\n\")\n",
    "        all_results['acceptance'] = None\n",
    "\n",
    "    print(\"\\\\n\")\n",
    "\n",
    "    # --- 2. Odds Ratio Analysis ---\n",
    "    print(f\"--- 2. Odds Ratio Analysis ({group2_label} vs {group1_label}) ---\")\n",
    "    g1_accepted = accepted_g1.sum()\n",
    "    g1_rejected = len(accepted_g1) - g1_accepted\n",
    "    \n",
    "    g2_accepted = accepted_g2.sum()\n",
    "    g2_rejected = len(accepted_g2) - g2_accepted\n",
    "\n",
    "    data = {\n",
    "        'accepted_count': [g2_accepted, g1_accepted],\n",
    "        'rejected_count': [g2_rejected, g1_rejected]\n",
    "    }\n",
    "    grouped_df = pd.DataFrame(data, index=[group2_label, group1_label])\n",
    "    \n",
    "    print(\"Grouped Data:\")\n",
    "    display(grouped_df)\n",
    "    \n",
    "    odds_results = analyze_odds_ratio(\n",
    "        grouped_df, \n",
    "        agent_label=group2_label, \n",
    "        human_label=group1_label\n",
    "    )\n",
    "    all_results['odds_ratio'] = odds_results\n",
    "    print(\"\\\\n\")\n",
    "\n",
    "    # --- 3. Duration Analysis (Mann-Whitney U) ---\n",
    "    print(f\"--- 3. Closed PR Duration Analysis ({group2_label} vs {group1_label}) ---\")\n",
    "    \n",
    "    # Filter for closed PRs\n",
    "    closed_df = df[df[state_col] == closed_value].copy()\n",
    "    \n",
    "    if closed_df.empty:\n",
    "        print(\"No closed PRs available to analyze duration.\\\\n\")\n",
    "        all_results['duration'] = None\n",
    "    else:\n",
    "        # Calculate duration\n",
    "        closed_df['created_at_dt'] = pd.to_datetime(closed_df[created_at_col], errors='coerce')\n",
    "        closed_df['closed_at_dt'] = pd.to_datetime(closed_df[closed_at_col], errors='coerce')\n",
    "        closed_df['pr_duration(h)'] = (closed_df['closed_at_dt'] - closed_df['created_at_dt']).dt.total_seconds() / 3600\n",
    "        \n",
    "        # Check normality (on the total set of closed durations)\n",
    "        print(\"Normality Check (Total Duration of Closed PRs):\")\n",
    "        check_normality(closed_df['pr_duration(h)'].dropna())\n",
    "        print(\"\\\\n\")\n",
    "\n",
    "        # Apply masks to the filtered dataframe\n",
    "        # Use .index.intersection to ensure we only get IDs present in closed_df\n",
    "        g1_indices = closed_df.index.intersection(group1_mask[group1_mask].index)\n",
    "        g2_indices = closed_df.index.intersection(group2_mask[group2_mask].index)\n",
    "\n",
    "        duration_g1 = closed_df.loc[g1_indices, 'pr_duration(h)'].dropna()\n",
    "        duration_g2 = closed_df.loc[g2_indices, 'pr_duration(h)'].dropna()\n",
    "\n",
    "        if duration_g1.empty or duration_g2.empty:\n",
    "            print(\"Not enough duration data for one or both groups.\\n\")\n",
    "            all_results['duration'] = None\n",
    "        else:\n",
    "            print(f\"Comparing Duration: {group2_label} ({len(duration_g2)} PRs) vs {group1_label} ({len(duration_g1)} PRs)\")\n",
    "            duration_results = compare_continuous_non_parametric(\n",
    "                duration_g2,  # g2 (e.g., Agent) first\n",
    "                duration_g1,  # g1 (e.g., Human) second\n",
    "                group2_label, \n",
    "                group1_label\n",
    "            )\n",
    "            all_results['duration'] = duration_results\n",
    "    \n",
    "    print(f\"\\\\n--- END OF ANALYSIS: {analysis_title} ---\\n\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 3: Usage Example\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Load and prepare your data\n",
    "#    !!!! IMPORTANT: Replace the file path with the correct one. !!!!\n",
    "path_to_file = r'output_files\\\\fix_prs_with_issues_and_files_and_tests.parquet'\n",
    "fixes_with_issues = pd.read_parquet(path_to_file)\n",
    "\n",
    "# Basic Preparation\n",
    "fixes_with_issues.rename(columns={'agent': 'user_type'}, inplace=True)\n",
    "fixes_with_issues['accepted'] = fixes_with_issues['merged_at'].notnull().astype(int)\n",
    "\n",
    "closed_prs = fixes_with_issues[fixes_with_issues['state'] != 'open']\n",
    "\n",
    "print(f\"DataFrame loaded successfully: {len(closed_prs)} rows.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Scenario 1: Agents vs Humans (in all 'fix' PRs)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Define masks for groups\n",
    "mask_human = closed_prs['user_type'] == 'Human'\n",
    "mask_agent = closed_prs['user_type'] != 'Human'\n",
    "\n",
    "# Call master function\n",
    "results_agent_vs_human = run_full_comparison(\n",
    "    df=closed_prs, \n",
    "    group1_mask=mask_human, \n",
    "    group2_mask=mask_agent, \n",
    "    group1_label='Human', \n",
    "    group2_label='Agents', \n",
    "    analysis_title=\"Analysis 1: Agents vs Humans (All 'fix' PRs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c473482-9873-494b-b13e-011756c9e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scenario 2: With Test vs Without Test (in all 'fix' PRs)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Define masks\n",
    "# Note: 'has_modified_test' is boolean\n",
    "mask_with_test = closed_prs['has_modified_test'] == True\n",
    "mask_without_test = closed_prs['has_modified_test'] == False\n",
    "\n",
    "# Call master function\n",
    "# Note: 'PRs without tests' is group 1 (human_label/control equivalent)\n",
    "# and 'PRs with tests' is group 2 (agent_label equivalent)\n",
    "results_test_vs_notest = run_full_comparison(\n",
    "    df=closed_prs,\n",
    "    group1_mask=mask_without_test,\n",
    "    group2_mask=mask_with_test,\n",
    "    group1_label='PRs without tests',\n",
    "    group2_label='PRs with tests',\n",
    "    analysis_title=\"Analysis 2: With Test vs Without Test (All 'fix' PRs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ca29d-8cb6-42d4-9b25-e72debe666e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scenario 3: Agents vs Humans (ONLY in PRs WITH TESTS)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# 1. Create filtered DataFrame first\n",
    "df_with_tests_only = closed_prs[closed_prs['has_modified_test'] == True].copy()\n",
    "print(f\"\\\\nFiltering for PRs with tests: {len(df_with_tests_only)} rows.\")\n",
    "\n",
    "# 2. Define masks on THIS NEW DataFrame\n",
    "mask_human_wt = df_with_tests_only['user_type'] == 'Human'\n",
    "mask_agent_wt = df_with_tests_only['user_type'] != 'Human'\n",
    "\n",
    "# 3. Call master function\n",
    "results_agent_vs_human_wt = run_full_comparison(\n",
    "    df=df_with_tests_only,\n",
    "    group1_mask=mask_human_wt,\n",
    "    group2_mask=mask_agent_wt,\n",
    "    group1_label='Human',\n",
    "    group2_label='Agents',\n",
    "    analysis_title=\"Analysis 3: Agents vs Humans (Only 'fix' PRs WITH TESTS)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71ad70-5b52-4e0f-9bd4-7811ff40a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scenario 4: With Issues vs Without Issues (in all 'fix' PRs)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "fixes_with_linked_issues = closed_prs[closed_prs['has_issues'] == True]\n",
    "\n",
    "# Define masks\n",
    "mask_Human = fixes_with_linked_issues['user_type'] == 'Human'\n",
    "mask_Agents = fixes_with_linked_issues['user_type'] != 'Human'\n",
    "\n",
    "# Call master function\n",
    "results_issues = run_full_comparison(\n",
    "    df=fixes_with_linked_issues,\n",
    "    group1_mask=mask_Human,\n",
    "    group2_mask=mask_Agents,\n",
    "    group1_label='Human PRs',\n",
    "    group2_label='Agent PRs',\n",
    "    analysis_title=\"Analysis 4: With Linked Issues - Agents vs Humans (All 'fix' PRs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5691395-920b-4b08-9e85-04aad4220d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\\\n=========================================================\")\n",
    "print(\"STARTING: Analysis 5: Acceptance Rate by Agent vs. Humans\")\n",
    "print(\"=========================================================\\\\n\")\n",
    "\n",
    "# 1. Get Human acceptance series\n",
    "human_acceptance = closed_prs.loc[mask_human, 'accepted']\n",
    "\n",
    "# 2. Get unique agent types list\n",
    "agent_types = closed_prs[~mask_human]['user_type'].unique()\n",
    "\n",
    "print(f\"Comparing {len(agent_types)} agent types against 'Human' ({len(human_acceptance)} PRs)...\")\n",
    "\n",
    "results_by_agent = []\n",
    "\n",
    "# 3. Iterate through each agent type\n",
    "for agent_name in agent_types:\n",
    "    mask_current_agent = closed_prs['user_type'] == agent_name\n",
    "    agent_acceptance = closed_prs.loc[mask_current_agent, 'accepted']\n",
    "    \n",
    "    print(f\"\\\\n--- Comparing {agent_name} ({len(agent_acceptance)} PRs) vs Human ---\")\n",
    "    \n",
    "    # 4. Call ORIGINAL function (from Block 1)\n",
    "    result_df = compare_acceptance_rates(\n",
    "        agent_acceptance,\n",
    "        human_acceptance,\n",
    "        label_1=agent_name,\n",
    "        label_2='Human'\n",
    "    )\n",
    "    \n",
    "    if result_df is not None:\n",
    "        result_df['agent_type'] = agent_name\n",
    "        result_df['agent_PRs'] = len(agent_acceptance)\n",
    "        result_df['agent_accepted_count'] = agent_acceptance.sum()\n",
    "        results_by_agent.append(result_df)\n",
    "\n",
    "# 5. Concatenate and process results\n",
    "if results_by_agent:\n",
    "    final_results_df = pd.concat(results_by_agent, ignore_index=True, sort=False)\n",
    "    \n",
    "    # a. Find all agent rate columns\n",
    "    agent_rate_cols = [col for col in final_results_df.columns if 'accept_rate' in col and 'Human' not in col]\n",
    "    \n",
    "    # b. Create unique 'agent_accept_rate' column by summing values\n",
    "    final_results_df['agent_accept_rate'] = final_results_df[agent_rate_cols].sum(axis=1)\n",
    "    final_results_df['agent_rejected_count'] = final_results_df['agent_PRs'] - final_results_df['agent_accepted_count']\n",
    "    \n",
    "    # c. Drop original dynamic agent rate columns\n",
    "    final_results_df = final_results_df.drop(columns=agent_rate_cols)\n",
    "\n",
    "    # 6. Define final column order\n",
    "    cols_order = [\n",
    "        'agent_type', \n",
    "        'agent_PRs', \n",
    "        'agent_accepted_count',\n",
    "        'agent_rejected_count',\n",
    "        'agent_accept_rate',\n",
    "        'Human_accept_rate',\n",
    "        'z_stat', \n",
    "        'p_value', \n",
    "        'significant', \n",
    "        'interpretation', \n",
    "        'absolute_difference'\n",
    "    ]\n",
    "    \n",
    "    # Filter for columns that actually exist\n",
    "    final_cols = [col for col in cols_order if col in final_results_df.columns]\n",
    "\n",
    "    style_format = {\n",
    "        'agent_accept_rate': \"{:.2%}\",\n",
    "        'Human_accept_rate': \"{:.2%}\",\n",
    "        \"z_stat\": \"{:.2f}\",\n",
    "        \"p_value\": \"{:.2e}\",\n",
    "        \"absolute_difference\": \"{:.2%}\",\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\n\\\\n--- Consolidated Result: Acceptance by Agent vs. Humans ---\")\n",
    "    \n",
    "    display(final_results_df[final_cols].sort_values('z_stat').style.format(style_format))\n",
    "else:\n",
    "    print(\"\\\\nNo agent comparison could be completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcdfb5-819e-4a28-bc40-04330f9f36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 8: 6x6 Comparison Matrix (Odds Ratio + Confidence Interval)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\\\n=========================================================\")\n",
    "print(\"STARTING: Analysis 6: 6x6 Comparison Matrices (with 95% CI)\")\n",
    "print(\"=========================================================\\\\n\")\n",
    "\n",
    "# 1. Get full list of user types (Human first)\n",
    "human_mask = closed_prs['user_type'] == 'Human'\n",
    "agent_types = sorted(list(closed_prs[~human_mask]['user_type'].unique()))\n",
    "user_types_list = ['Human'] + agent_types\n",
    "\n",
    "print(f\"Comparing the following groups: {user_types_list}\")\n",
    "\n",
    "# 2. Pre-calculate counts\n",
    "try:\n",
    "    grouped_counts = closed_prs.groupby('user_type')['accepted'].agg(\n",
    "        accepted_count=lambda x: x.sum(),\n",
    "        total_count=lambda x: x.count()\n",
    "    )\n",
    "    grouped_counts['rejected_count'] = grouped_counts['total_count'] - grouped_counts['accepted_count']\n",
    "\n",
    "    # 3. Initialize matrices\n",
    "    df_odds_ratios = pd.DataFrame(np.nan, index=user_types_list, columns=user_types_list, dtype=float)\n",
    "    df_p_values = pd.DataFrame(np.nan, index=user_types_list, columns=user_types_list, dtype=float)\n",
    "    \n",
    "    # New matrices for Confidence Interval\n",
    "    df_ci_lower = pd.DataFrame(np.nan, index=user_types_list, columns=user_types_list, dtype=float)\n",
    "    df_ci_upper = pd.DataFrame(np.nan, index=user_types_list, columns=user_types_list, dtype=float)\n",
    "    df_ci_formatted = pd.DataFrame(\"\", index=user_types_list, columns=user_types_list, dtype=object)\n",
    "\n",
    "    # 4. Iterate through each pair\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        for user_X in user_types_list: # Interest Group\n",
    "            for user_Y in user_types_list: # Reference Group\n",
    "                \n",
    "                # Diagonal\n",
    "                if user_X == user_Y:\n",
    "                    df_odds_ratios.loc[user_X, user_Y] = 1.0\n",
    "                    df_p_values.loc[user_X, user_Y] = 1.0\n",
    "                    df_ci_lower.loc[user_X, user_Y] = 1.0\n",
    "                    df_ci_upper.loc[user_X, user_Y] = 1.0\n",
    "                    df_ci_formatted.loc[user_X, user_Y] = \"[1.00, 1.00]\"\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Build 2x2 table\n",
    "                    a = grouped_counts.loc[user_X, 'accepted_count']\n",
    "                    b = grouped_counts.loc[user_X, 'rejected_count']\n",
    "                    c = grouped_counts.loc[user_Y, 'accepted_count']\n",
    "                    d = grouped_counts.loc[user_Y, 'rejected_count']\n",
    "                    \n",
    "                    contingency_table = [[a, b], [c, d]]\n",
    "                    ct = Table2x2(contingency_table)\n",
    "                    \n",
    "                    # Fill OR and P-Value\n",
    "                    df_odds_ratios.loc[user_X, user_Y] = ct.oddsratio\n",
    "                    df_p_values.loc[user_X, user_Y] = ct.oddsratio_pvalue()\n",
    "                    \n",
    "                    # --- NEW: Calculate Confidence Interval (95%) ---\n",
    "                    ci_low, ci_upp = ct.oddsratio_confint(alpha=0.05)\n",
    "                    \n",
    "                    df_ci_lower.loc[user_X, user_Y] = ci_low\n",
    "                    df_ci_upper.loc[user_X, user_Y] = ci_upp\n",
    "                    \n",
    "                    # Formatting for friendly display: \"[Low, High]\"\n",
    "                    df_ci_formatted.loc[user_X, user_Y] = f\"[{ci_low:.2f}, {ci_upp:.2f}]\"\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error comparing {user_X} vs {user_Y}: {e}\")\n",
    "                    \n",
    "    # 7. Display results\n",
    "    print(\"\\n--- Odds Ratios Matrix ---\")\n",
    "    display(df_odds_ratios.style.format(\"{:.3f}\").background_gradient(cmap='coolwarm', vmin=0, vmax=2))\n",
    "\n",
    "    print(\"\\n--- P-Values Matrix (Bold if < 0.05) ---\")\n",
    "    display(df_p_values.style.format(\"{:.2e}\").map(lambda x: 'font-weight: bold' if x < 0.05 else ''))\n",
    "\n",
    "    print(\"\\n--- 95% Confidence Intervals Matrix ---\")\n",
    "    print(\"Interpretation: If the interval crosses 1.0 (e.g., [0.8, 1.2]), the association is not statistically significant.\")\n",
    "    display(df_ci_formatted)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"A general error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb43b56-8cbe-47cd-8d20-465b99b49f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_results_df[['agent_type','agent_accept_rate','Human_accept_rate','z_stat',\n",
    "                          'p_value','significant','interpretation']].sort_values('z_stat').style.format(style_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5f77a-f6f2-42a4-8f7f-d1cce67aae4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
