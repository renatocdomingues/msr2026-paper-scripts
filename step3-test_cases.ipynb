{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306867fa-fa2d-4e2b-8ff8-c71a7039af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe16fa-9210-4ee2-8d7b-d45936e8abd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prs = pd.read_parquet(r'output_files\\fix_prs_with_issues_and_files.parquet')\n",
    "prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fed11-22f4-4004-a8d9-7052dbd766a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_patch_e_converter(item):\n",
    "    \"\"\"\n",
    "    Removes the \"patch\" field (which is corrupted) from the string\n",
    "    and then converts it to a list.\n",
    "    \"\"\"\n",
    "    if not isinstance(item, str):\n",
    "        return item\n",
    "    \n",
    "    item_limpo = item.strip()\n",
    "    if item_limpo == \"\":\n",
    "        return []\n",
    "\n",
    "    # --- THE MOST IMPORTANT STEP ---\n",
    "    # This regex finds \"patch\": null OR \"patch\": \"...\"\n",
    "    # and correctly handles escaped quotes (\\\"...\\\") inside the patch.\n",
    "    # It replaces the field with \"patch\": null, which is safe to parse.\n",
    "    \n",
    "    # Regex to find \"patch\": \"...\" (handling escapes)\n",
    "    regex_string_patch = r'\"patch\":\\s*\"(?:[^\"\\\\]|\\\\.)*\"'\n",
    "    # Regex to find \"patch\": null\n",
    "    regex_null_patch = r'\"patch\":\\s*null'\n",
    "    \n",
    "    # Combines both:\n",
    "    regex_full = f\"({regex_string_patch}|{regex_null_patch})\"\n",
    "    \n",
    "    # Replaces what was found with a harmless value\n",
    "    item_sem_patch = re.sub(regex_full, '\"patch\": null', item_limpo)\n",
    "    \n",
    "    # 1. Fix Python literals -> JSON\n",
    "    item_corrigido = re.sub(r'\\bNone\\b', 'null', item_sem_patch, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\bTrue\\b', 'true', item_corrigido, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\bFalse\\b', 'false', item_corrigido, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\bnan\\b', 'null', item_corrigido, flags=re.IGNORECASE)\n",
    "\n",
    "    # 2. Try to decode as JSON\n",
    "    try:\n",
    "        return json.loads(item_corrigido)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # If it fails even without the patch, the string is 100% lost\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d280d-591d-4560-b034-c435a3ac4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter_para_lista_json_robusta(item):\n",
    "    \"\"\"\n",
    "    Converts a string (JSON-like) to a list.\n",
    "    Handles Python literals (None, True, nan) that might be mixed in.\n",
    "    Returns np.nan if the string is 100% corrupted.\n",
    "    \"\"\"\n",
    "    # 1. If it is not a string (already a list, or real NaN), just return\n",
    "    if not isinstance(item, str):\n",
    "        return item\n",
    "    \n",
    "    # 2. If it is a string, first remove whitespace\n",
    "    item_limpo = item.strip()\n",
    "    \n",
    "    # 3. If it is an empty string, return an empty list []\n",
    "    if item_limpo == \"\":\n",
    "        return []\n",
    "        \n",
    "    # --- CORRECTION STEP (Python -> JSON) ---\n",
    "    item_corrigido = re.sub(r'\\bNone\\b', 'null', item_limpo, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\bTrue\\b', 'true', item_corrigido, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\bFalse\\b', 'false', item_corrigido, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\bnan\\b', 'null', item_corrigido, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\binf\\b', 'null', item_corrigido, flags=re.IGNORECASE)\n",
    "    item_corrigido = re.sub(r'\\b-inf\\b', 'null', item_corrigido, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. Try to decode as JSON\n",
    "    try:\n",
    "        return json.loads(item_corrigido)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # This is where the 144 corrupted lines (with patch) will fall\n",
    "        return np.nan # Return NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2d397-ea49-48ca-8642-04bd52008517",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs['modified_files_list'] = prs['modified_files'].apply(converter_para_lista_json_robusta)\n",
    "\n",
    "# --- STEP 1: Perform explode() ---\n",
    "df_exploded = prs.explode('modified_files_list')\n",
    "#display(df_exploded)\n",
    "\n",
    "# --- STEP 2: Normalize (expand the dict) ---\n",
    "normalized_cols = pd.json_normalize(df_exploded['modified_files_list'])\n",
    "normalized_cols.index = df_exploded.index\n",
    "\n",
    "# --- STEP 3: Join everything ---\n",
    "prs_and_changes = pd.concat(\n",
    "    [df_exploded.drop(['modified_files'], axis=1), normalized_cols],\n",
    "    axis=1\n",
    ")\n",
    "prs_and_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5e7ad-a0db-4842-8c5f-df78c8d8c1a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- 1. REGEX TO IDENTIFY TEST FILES ---\n",
    "# (Corrected: '(?_tests__' -> '(?:__tests__')\n",
    "filename_regex = (\n",
    "    r'('\n",
    "    r'(?:^tests[/\\\\])'                     # starts with tests/\n",
    "    r'|(?:[/\\\\]tests?[/\\\\])'               # contains /tests/ or /test/\n",
    "    r'|(?:[/\\\\]test[/\\\\])'                 # contains /test/\n",
    "    r'|(?:__tests__[/\\\\])'                 # __tests__/  <-- CORRECTION IS HERE\n",
    "    r'|(?:\\.spec\\b)'                       # .spec (ex: index.spec.tsx)\n",
    "    r'|(?:\\_test\\.)'                       # _test. (ex: foo_test.py)\n",
    "    r'|(?:\\.test\\.)'                       # .test. (ex: utils.test.js)\n",
    "    r'|(?:src[/\\\\]test[/\\\\])'              # src/test/ (Java/Kotlin)\n",
    "    r'|(?:[/\\\\](?:unit[-_]?tests?|integration[-_]?tests?)[/\\\\])' # /unit-tests/\n",
    "    r'|(?:\\_spec\\.)'                       # _spec. (Ruby, etc.)\n",
    "    r'|(?:[A-Za-z0-9_]+(?:Test|Tests|Spec)\\.[a-z0-9]+$)' # NameTest.ext\n",
    "    r'|(?:Test\\.(?:php|java|cs|kt|ts|tsx|py|go|rb))'  # Test.php, Test.java, ...\n",
    "    r')'\n",
    ")\n",
    "\n",
    "# --- 2. REGEX TO IGNORE CONFIG/LOCK/DOC FILES ---\n",
    "config_regex = (\n",
    "    r'('\n",
    "    # 1. Lock and dependency files (exact names at end of string)\n",
    "    r'(?:package-lock\\.json$|yarn\\.lock$|composer\\.lock$|requirements\\.txt$)'\n",
    "    \n",
    "    # 2. Common extensions for config, data, or documentation\n",
    "    r'|(?:.(json|ya?ml|xml|ini|toml|conf(ig)?|lock|log|md|txt)$)'\n",
    "    \n",
    "    # 3. Dotfiles (files starting with ., ex: .gitignore, .prettierrc)\n",
    "    r'|([/\\\\]\\.[^/\\\\]+$)'\n",
    "    r')'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. APPLYING THE LOGIC ---\n",
    "\n",
    "# Step 1: Mark everything that looks like a test\n",
    "is_test_file = prs_and_changes['filename'].str.contains(\n",
    "    filename_regex, \n",
    "    case=False,\n",
    "    na=False\n",
    ")\n",
    "\n",
    "# Step 2: Mark everything that is a config/documentation file\n",
    "is_config_file = prs_and_changes['filename'].str.contains(\n",
    "    config_regex, \n",
    "    case=False,\n",
    "    na=False\n",
    ")\n",
    "\n",
    "# Step 3: The final rule.\n",
    "prs_and_changes['is_filename_a_test_file'] = is_test_file\n",
    "prs_and_changes['is_filename_a_config_file'] = is_config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0129c-6049-41ac-8d74-f1b689d2194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_pattern = re.compile(\n",
    "    r'(def\\s+test_|pytest\\b|unittest\\b|assert\\b|describe\\s*\\(|\\bit\\s*\\(|\\btest\\(|\\bexpect\\(|@Test\\b|func\\s+Test\\b)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def is_test_by_content(patch):\n",
    "    # 1. Check for non-strings:\n",
    "    #    - pd.isna() catches None and np.nan\n",
    "    #    - not isinstance(patch, str) catches numbers, lists, etc.\n",
    "    if pd.isna(patch) or not isinstance(patch, str):\n",
    "        return False\n",
    "    \n",
    "    # 2. Check for empty strings (what 'not patch' did)\n",
    "    if not patch:\n",
    "        return False\n",
    "        \n",
    "    # 3. If it is a valid string, perform the search\n",
    "    return bool(content_pattern.search(patch))\n",
    "\n",
    "# --- CORRECTED APPLICATION ---\n",
    "# Remove .fillna('') - the function now handles everything.\n",
    "prs_and_changes['is_patch_a_test_file'] = prs_and_changes['patch'].apply(is_test_by_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588bc7e6-5805-4719-b9f1-ae4f9b856831",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_and_changes['is_test_file'] = ~prs_and_changes['is_filename_a_config_file'] & \\\n",
    "(prs_and_changes['is_filename_a_test_file'] | prs_and_changes['is_patch_a_test_file'])\n",
    "\n",
    "\n",
    "# Optional: see how many were detected by each heuristic\n",
    "print('Total Files:',len(prs_and_changes))\n",
    "print(\"Config file identified by name:\", prs_and_changes['is_filename_a_config_file'].sum())\n",
    "print(\"Count of test files (excluding configs):\")\n",
    "print(\"Test identified by name:\", prs_and_changes['is_filename_a_test_file'].sum())\n",
    "\n",
    "print(\"Test identified by patch:\", prs_and_changes['is_patch_a_test_file'].sum(),'<- Generating False Positives, should we keep this?')\n",
    "\n",
    "print(\"Combined:\", prs_and_changes['is_test_file'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77caae-0565-4ebb-83d1-e3fb22961dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prs_and_changes[prs_and_changes['is_test_file'] == True].groupby(['id','number','repo_url','status'])\\\n",
    "                                                                            .count().reset_index()\\\n",
    "                                                                            [['id','number','repo_url','status','user']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bcc07-a0e1-44b0-b771-4e3ca4c2e999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_tests = prs_and_changes[prs_and_changes['is_test_file'] == True].groupby(['id','number','repo_url','status'])\\\n",
    "                                                                            .count().reset_index()\\\n",
    "                                                                            [['id','number','repo_url','status','user']]\n",
    "number_of_tests.rename(columns={\"user\": \"quantity\"},inplace = True)\n",
    "number_of_tests.sort_values('quantity')\n",
    "prs['#_of_files'] = prs['modified_files_list'].str.len()\n",
    "\n",
    "# 1. Use pivot_table to perform the transformation\n",
    "pivoted_number_of_tests = number_of_tests.pivot_table(\n",
    "    index=['id', 'number', 'repo_url'],  # Columns identifying the row\n",
    "    columns='status',                   # Column whose values will become new columns\n",
    "    values='quantity',                  # Column whose values will fill the cells\n",
    "    aggfunc='sum',                      # What to do if duplicates exist (sum)\n",
    "    fill_value=0                        # Replace NaNs with 0\n",
    ")\n",
    "\n",
    "# 2. Add the suffix '_tests' to the new column names (x, y, z)\n",
    "pivoted_number_of_tests = pivoted_number_of_tests.add_suffix('_tests')\n",
    "\n",
    "# 3. Cleanup: 'flatten' the column index and bring the index back\n",
    "df_final = pivoted_number_of_tests.rename_axis(columns=None).reset_index()\n",
    "\n",
    "# 1. Perform the merge. This will create the DataFrame with NaNs\n",
    "merged_df = pd.merge(prs, df_final, on=['id', 'number', 'repo_url'], how='left')\n",
    "\n",
    "# 2. Fill NaNs with 0 ONLY in the specific column(s)\n",
    "# (Replace 'count_column_name' with your actual column name)\n",
    "merged_df[['added_tests','modified_tests','removed_tests','renamed_tests']] = merged_df[['added_tests','modified_tests',\n",
    "                                                                                                     'removed_tests','renamed_tests']].fillna(0)\n",
    "\n",
    "# (Optional) If you want the column to be integers (instead of float 6.0)\n",
    "merged_df[['added_tests','modified_tests','removed_tests','renamed_tests']] = merged_df[['added_tests','modified_tests',\n",
    "                                                                                                     'removed_tests','renamed_tests']].astype(int)\n",
    "prs_with_files = merged_df[merged_df['#_of_files'] > 0]\n",
    "prs_without_files = merged_df[merged_df['#_of_files'] == 0]\n",
    "\n",
    "colunas_de_teste = ['added_tests', 'modified_tests', 'removed_tests']\n",
    "\n",
    "# 1. Sum the columns (treat NaNs as 0)\n",
    "soma_testes = merged_df[colunas_de_teste].sum(axis=1)\n",
    "\n",
    "# 2. Create the final column (True if sum is greater than 0)\n",
    "merged_df['has_modified_test'] = (soma_testes > 0)\n",
    "\n",
    "merged_df.to_parquet(r'output_files\\fix_prs_with_issues_and_files_and_tests.parquet')\n",
    "merged_df.sort_values('#_of_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44184fbb-5b1a-49c0-857f-31d7298f594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_with_test_list = prs_and_changes[prs_and_changes['is_test_file'] == True]['id'].unique().tolist()\n",
    "prs_with_identified_test = prs[prs['id'].isin(prs_with_test_list)]\n",
    "prs_without_identified_test = prs[~prs['id'].isin(prs_with_test_list)]\n",
    "print('Total PRs & fix & issues:',len(prs))\n",
    "print('Total PRs with files:',len(prs_with_files))\n",
    "print(\"Total PRs without files:\", len(prs_without_files))\n",
    "print('Total PRs of fix & issues & tests:',len(prs_with_identified_test))\n",
    "print(\"Total PRs of fix & issues & ~tests:\", len(prs_without_identified_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9bc4a-d21d-406f-a90a-e538bfae7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['modified_files_list'] = merged_df['modified_files'].apply(converter_para_lista_json_robusta)\n",
    "\n",
    "def explode_df(merged_df):\n",
    "    extensions_df_exploded = merged_df.explode('modified_files_list')\n",
    "    extensions_normalized_cols = pd.json_normalize(extensions_df_exploded['modified_files_list'])\n",
    "    extensions_normalized_cols.index = extensions_df_exploded.index\n",
    "    to_be_extensions_df = pd.concat(\n",
    "        [extensions_df_exploded.drop(['modified_files'], axis=1), extensions_normalized_cols],\n",
    "        axis=1\n",
    "    )\n",
    "    return to_be_extensions_df\n",
    "    \n",
    "to_be_extensions_df = explode_df(merged_df)\n",
    "to_be_extensions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d15c9-897a-414b-ba9f-9e67356ee2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_extensao(caminho):\n",
    "    # Checks if it is a string (ignores NaNs, Nones, etc.)\n",
    "    if isinstance(caminho, str):\n",
    "        # Gets the extension part (ex: '.py')\n",
    "        return os.path.splitext(caminho)[1]\n",
    "    # Returns None if input is not a string\n",
    "    return None\n",
    "\n",
    "def extensions_count(df):\n",
    "    df['extensao'] = df['filename'].apply(extrair_extensao)\n",
    "    extensions_count = df['extensao'].value_counts(dropna=False).reset_index()\n",
    "    extensions_count.columns = ['extensao', 'count']\n",
    "    extensions_count['extensao'] = extensions_count['extensao'].fillna('No file')\n",
    "    extensions_count['extensao'] = extensions_count['extensao'].replace('', 'No extension')\n",
    "    return extensions_count\n",
    "extensions_count_fix = extensions_count(to_be_extensions_df)\n",
    "extensions_count_fix.to_csv(r'output_files\\files_extesions_count.csv',index = False)\n",
    "extensions_count_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa80c36-1a4b-47b4-8048-930e2ab39095",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(extensions_count_fix[extensions_count_fix['extensao']== 'No extension'])\n",
    "display(extensions_count_fix[extensions_count_fix['extensao']== 'No file'])\n",
    "#extensions_count['count'].sum()\n",
    "extensions_count_fix['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf6fcf-ab75-4cbb-b7ab-b8d2211e5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_files = to_be_extensions_df[['filename','extensao']].drop_duplicates().groupby('extensao').count().reset_index()\n",
    "unique_files['extensao'] = unique_files['extensao'].fillna('No file')\n",
    "unique_files['extensao'] = unique_files['extensao'].replace('', 'No extension')\n",
    "unique_files.sort_values('filename')\n",
    "unique_files.sort_values('filename').to_csv(r'output_files\\unique_files_extesions_count.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e6991-bbd2-4285-aa20-0b12930daa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_test_df = explode_df(merged_df[merged_df['has_modified_test']])\n",
    "extensions_count_test = extensions_count(exploded_test_df)\n",
    "extensions_count_test.to_csv(r'output_files\\(test)_files_extesions_count.csv',index = False)\n",
    "display(extensions_count_test)\n",
    "display(extensions_count_test[extensions_count_test['extensao']== 'No extension'])\n",
    "extensions_count_test['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384e917-e6a5-48fe-9912-324ebd1f2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_files = exploded_test_df[['filename','extensao']].drop_duplicates().groupby('extensao').count().reset_index()\n",
    "test_unique_files['extensao'] = test_unique_files['extensao'].fillna('No file')\n",
    "test_unique_files['extensao'] = test_unique_files['extensao'].replace('', 'No extension')\n",
    "test_unique_files.sort_values('filename').to_csv(r'output_files\\(test)_unique_files_extesions_count.csv',index = False)\n",
    "#unique_files.sort_values('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8741587-c4a4-4683-9859-d8348300be70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
