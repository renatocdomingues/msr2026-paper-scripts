{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1f4b9-76f9-4c6e-8799-f467eb4fed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf1d17-238d-41ec-8c30-ec54b499b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"INSERT_TOKEN\"\n",
    "headers = {\"Authorization\": f\"token {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d502abb-881b-4a40-992b-a683ad7d93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input file (output from previous step)\n",
    "all_fix_prs_with_issues = pd.read_parquet('output_files/fix_PRs_with_issues.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3cf82-bad4-407c-977e-be32b174676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_owner_repo(url):\n",
    "    path = url.strip().replace(\"https://github.com/\", \"\").replace(\"https://api.github.com/repos/\", \"\")\n",
    "    parts = path.strip(\"/\").split(\"/\")\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2c36e-8273-4a64-89e5-bea9fccb2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to fetch modified files (with status and patch)\n",
    "def get_pr_files_detailed(owner, repo, pr_number, headers):\n",
    "    \"\"\"Returns a list of dictionaries: filename, status, and patch\"\"\"\n",
    "    files_info = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files?page={page}&per_page=100\"\n",
    "        r = requests.get(url, headers=headers)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Error {r.status_code} in PR {pr_number} ({owner}/{repo})\")\n",
    "            break\n",
    "        data = r.json()\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        for f in data:\n",
    "            files_info.append({\n",
    "                \"filename\": f.get(\"filename\"),\n",
    "                \"status\": f.get(\"status\"),\n",
    "                \"additions\": f.get(\"additions\"),\n",
    "                \"deletions\": f.get(\"deletions\"),\n",
    "                \"changes\": f.get(\"changes\"),\n",
    "                \"patch\": f.get(\"patch\")\n",
    "            })\n",
    "        page += 1\n",
    "    return files_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2e2c6-dbd0-4a57-87a3-8d54a2686116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_json_load(x):\n",
    "    \"\"\"\n",
    "    Helper function to safely load JSON from a parquet cell,\n",
    "    handling empty/NaN cells or malformed JSON.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return [] # Returns empty list if cell was NaN\n",
    "    \n",
    "    try:\n",
    "        # Tries to load as JSON (standard and correct format)\n",
    "        return json.loads(x)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # If it fails, tries as a Python 'literal' (e.g., \"['a', 'b']\")\n",
    "        # This can happen if the parquet was saved in a weird way\n",
    "        try:\n",
    "            import ast\n",
    "            result = ast.literal_eval(x)\n",
    "            # Ensures the result is indeed a list\n",
    "            return result if isinstance(result, list) else []\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If both fail, it is invalid data\n",
    "            print(f\"Warning: Failed to decode JSON/literal, treating as empty: {str(x)[:50]}...\")\n",
    "            return [] # Returns empty list if invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c78e4c-ec3e-4808-8c7c-df857d0c5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "save_interval = 50\n",
    "final_column_name = \"modified_files\" # Name of the result column\n",
    "partial_file_prefix = \"output_files/partial/prs_with_files_partial_\"\n",
    "final_output_file = \"output_files/prs_with_files_FINAL.parquet\" \n",
    "results = []\n",
    "start_index = 0\n",
    "df_final_found = False # Flag to skip the loop\n",
    "\n",
    "# ===================================================\n",
    "# 0. CHECK IF FINAL FILE ALREADY EXISTS\n",
    "# ===================================================\n",
    "if os.path.exists(final_output_file):\n",
    "    print(f\"üéâ Found final file: {final_output_file}\")\n",
    "    print(\"Loading final results and skipping loop...\")\n",
    "    \n",
    "    try:\n",
    "        # Load final DF\n",
    "        df_complete = pd.read_parquet(final_output_file)\n",
    "        \n",
    "        # Deserialize the column\n",
    "        df_complete[final_column_name] = df_complete[final_column_name].apply(safe_json_load)\n",
    "        \n",
    "        # Synchronize 'results' and 'start_index' to skip loop\n",
    "        results = df_complete[final_column_name].tolist()\n",
    "        start_index = len(df_complete) \n",
    "        \n",
    "        # IMPORTANT: Overwrite 'all_fix_prs_with_issues' with the complete version\n",
    "        all_fix_prs_with_issues = df_complete\n",
    "        df_final_found = True # Set the flag\n",
    "        \n",
    "        print(f\"‚úÖ Final results loaded. {len(results)} rows.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading final file {final_output_file}: {e}\")\n",
    "        print(\"Ignoring and continuing with partial resume logic...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1851c-0add-4893-bfdc-df06834107d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 1. SEARCH FOR PARTIAL FILE (if final was not loaded)\n",
    "# ===================================================\n",
    "if not df_final_found: # Only runs if final load failed or didn't exist\n",
    "    partial_files = glob.glob(f\"{partial_file_prefix}*.parquet\") \n",
    "\n",
    "    if partial_files:\n",
    "        latest_partial_file = max(partial_files, key=os.path.getmtime)\n",
    "        print(f\"Found partial save: {latest_partial_file}\")\n",
    "        \n",
    "        try:\n",
    "            df_partial = pd.read_parquet(latest_partial_file) \n",
    "            \n",
    "            # Deserialize JSON\n",
    "            results = df_partial[final_column_name].apply(safe_json_load).tolist()\n",
    "            start_index = len(results)                       \n",
    "            \n",
    "            print(f\"Loaded {start_index} previous results (from parquet).\")\n",
    "            print(f\"Resuming process from index {start_index}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing {latest_partial_file}: {e}. Starting from scratch.\")\n",
    "            results = []\n",
    "            start_index = 0\n",
    "    else:\n",
    "        print(\"No partial save found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2066874-9a37-4b36-9dcf-c72211553542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. We iterate only over what is MISSING\n",
    "df_remaining = all_fix_prs_with_issues.iloc[start_index:]\n",
    "\n",
    "print(f\"Processing {len(df_remaining)} remaining PRs (from {start_index} to {len(all_fix_prs_with_issues)})...\")\n",
    "\n",
    "# 5. We use 'start=start_index' in enumerate\n",
    "for i, (idx, row) in enumerate(tqdm(df_remaining.iterrows(), total=len(df_remaining)), start=start_index):\n",
    "    owner, repo = extract_owner_repo(row[\"repo_url\"])\n",
    "    pr_number = row[\"number\"]\n",
    "\n",
    "    try:\n",
    "        files = get_pr_files_detailed(owner, repo, pr_number, headers)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in PR {pr_number} ({owner}/{repo}): {e}\")\n",
    "        files = [] # Error value must be an empty list\n",
    "\n",
    "    # 1. Just collect into list:\n",
    "    results.append(files) # Adds the *new* result (still as Python list)\n",
    "    \n",
    "    # üíæ Partial save\n",
    "    if (i + 1) % save_interval == 0:\n",
    "        print(f\"\\nStarting partial save (processed {i+1})...\")\n",
    "        \n",
    "        # 2. CREATE PARTIAL DATAFRAME\n",
    "        df_partial = all_fix_prs_with_issues.iloc[:i+1].copy()  \n",
    "        \n",
    "        # 3. ASSIGN RESULT LIST\n",
    "        df_partial[final_column_name] = results\n",
    "        \n",
    "        # 4. SERIALIZE COLUMN TO JSON (NEW STEP!)\n",
    "        # Parquet cannot store lists, so we transform the list into a JSON string.\n",
    "        df_partial[final_column_name] = df_partial[final_column_name].apply(json.dumps)\n",
    "        \n",
    "        # 5. SAVE PARTIAL DF (to parquet)\n",
    "        partial_path = f\"{partial_file_prefix}{i+1}.parquet\" # CHANGED TO .parquet\n",
    "        df_partial.to_parquet(partial_path, index=False) # CHANGED TO .to_parquet\n",
    "        print(f\"‚úÖ Partial save: {partial_path}\")\n",
    "\n",
    "\n",
    "# --- Final save\n",
    "print(\"\\nLoop finished. Preparing final save...\")\n",
    "\n",
    "#display(results[0])\n",
    "\n",
    "# 5. FINAL ASSIGNMENT\n",
    "all_fix_prs_with_issues[final_column_name] = results\n",
    "# 6. FINAL SERIALIZATION (NEW STEP!)\n",
    "# We also need to serialize the final column before saving to parquet\n",
    "all_fix_prs_with_issues[final_column_name] = all_fix_prs_with_issues[final_column_name].apply(json.dumps)\n",
    "\n",
    "# 7. FINAL SAVE\n",
    "all_fix_prs_with_issues.to_parquet(final_output_file, index=False) # CHANGED TO .to_parquet\n",
    "print(f\"‚úÖ Final save complete: {final_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699da46-c2d0-4ecc-ab80-c7133e0ddcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# üßπ CLEANUP BLOCK \n",
    "# ===================================================\n",
    "print(f\"\\nCleaning partial files with prefix: '{partial_file_prefix}'...\")\n",
    "try:\n",
    "    # partial_file_prefix = \"prs_with_files_partial_\"\n",
    "    # Ensure the extension is correct (.parquet or .pkl)\n",
    "    pattern_to_clean = f\"{partial_file_prefix}*.parquet\" \n",
    "    \n",
    "    # If you are still using pickle for this script:\n",
    "    # pattern_to_clean = f\"{partial_file_prefix}*.pkl\" \n",
    "    \n",
    "    files_to_delete = glob.glob(pattern_to_clean)\n",
    "    \n",
    "    if not files_to_delete:\n",
    "        print(\"No partial files to clean.\")\n",
    "    else:\n",
    "        for f in files_to_delete:\n",
    "            os.remove(f)\n",
    "        print(f\"‚úÖ {len(files_to_delete)} partial files removed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error during partial file cleanup: {e}\")\n",
    "# ===================================================\n",
    "# END OF CLEANUP BLOCK\n",
    "# ===================================================\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf41083-5f1e-40e0-9627-9318d20ca09c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#460 human\n",
    "#all_fix_prs_with_issues.to_parquet(r'output_files\\fix_prs_with_issues_and_files.parquet', index=False) #<- may generate errors in modified files\n",
    "all_fix_prs_with_issues.to_parquet(r'output_files\\fix_prs_with_issues_and_files.parquet')\n",
    "all_fix_prs_with_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bb27d-4ec1-4820-9b97-936ce7d9e0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
